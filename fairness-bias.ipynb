{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca64366-3cb3-472c-9bb7-d3a1cd9f62c6",
   "metadata": {},
   "source": [
    "### Sensitive Attribute (z) is age.\n",
    "\n",
    "We chose age as our sensitive attribute because age correlates with differing financial behaviors and needs. For example, younger individuals typically require more liquidity and are less likely to commit to long-term savings products like bank term deposits due to their dynamic financial circumstances and greater likelihood of needing immediate access to cash. On the other hand, older individuals often have more financial stability and may be more inclined to lock in funds for a higher return. Additionally, many of the other features such as marital status, job type, if the customer has a housing loan or not, and much more are correlated to age.\n",
    "\n",
    "This naturally creates a potential for unfairness where models trained on historical data might inadvertently favor older clients while neglecting younger ones—even if the underlying behavior differences are statistically true on average. By focusing on age, we can analyze and quantify these disparities (e.g., through differences in true positive rates) and then apply fairness interventions to ensure that the model does not systematically disadvantage younger clients, making our results both interesting and practically relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee26f4c-072f-406b-9751-25aa10e7e896",
   "metadata": {},
   "source": [
    "### Fairness Metric - Equal Opportunity\n",
    "We chose equal opportunity as our primary fairness metric because it directly measures the model's ability to correctly identify positive cases (i.e., clients who subscribe to a term deposit) across different groups. From the perspective of the bank, we want to get as many customers to subscribe to a term deposit as possible. If a person would be interested to a term deposit, we want to ensure that we speak to them to secure the deposit. In contrast, it's OK if this comes a little bit at a cost of an increase in failed calls. In terms of our model, we want to increase the true positive rate (TPR) of the underserved group to match the better-served group. Ideally, we achieve this with minimal loss in accuracy (to prevent wasted time or marketing efforts).\n",
    "\n",
    "In our context, ensuring that the TPR is as close as possible for both the better-served group (e.g., older clients) and the underserved group (younger clients) is critical for guaranteeing equal opportunity. A low TPR difference indicates that clients with a genuine likelihood to subscribe are equally recognized by the model, regardless of their age. This metric is particularly relevant because misclassifying a true positive (especially for a group that might already be under-targeted) can lead to missed opportunities and potential discrimination. Using a ratio of the TPRs allows us to quantify fairness in a way that addresses the core objective of our predictive task.\n",
    "\n",
    "Misclassifying a client who is likely to subscribe (a false negative) can mean denying a beneficial financial product to that individual, which is especially problematic if it happens disproportionately for one age group. By focusing on the ratio of the TPRs, we ensure that both older (protected) and younger (unprotected) clients who are truly eligible for the product are equally likely to be identified by the model, which maximizes the effects of the marketing campaign. Our argument emphasizes that, given the potential consequences for customer access and fairness in financial services, using the ratio of TPRs or equal opportunity as our fairness metric is not only justified but essential for the responsible deployment of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733eec9c-0572-4b34-9ac1-c43a48c3fcdd",
   "metadata": {},
   "source": [
    "### Data Preparation + Base Model\n",
    "\n",
    "We import the necessary libraries (pandas, NumPy, scikit-learn, etc.) and load our data `bank-full.csv`. For data pre-processing, we simply just converted all categorical data into one-hot-encodings and scaled numerical values for faster convergence. We then converts the label to binary indicating whether a client subscribed to a term deposit or not and then created the sensitive attribute z based on age. To create a binary variable and separate the \"older\" group from the \"younger\" group, we split on the 75th percentile ($z=0$ if $\\text{age} \\geq 48$, $z=1$ otherwise) The data is split into training and test sets. \n",
    "\n",
    "We used a logstic regression model as our baseline and define a helper function for calculating the True Positive Rate (TPR) to measure equal opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age threshold: 48.0\n",
      "Train set size: 36168\n",
      "Test set size: 9043\n",
      "Number of older customers (z=0) in dataset: 12185\n",
      "Number of younger customers (z=1) in dataset: 33026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data_path = \"data/bank-full.csv\"\n",
    "df_original = pd.read_csv(data_path, sep=\";\")\n",
    "df = df_original.copy()\n",
    "\n",
    "# Obtain the 75th percentile of the age column to use as the threshold for the sensitive attribute\n",
    "age_threshold = df[\"age\"].quantile(0.75)\n",
    "print(\"Age threshold:\", age_threshold)\n",
    "\n",
    "# Convert the target variable to binary (0 = No, 1 = Yes)\n",
    "df[\"y\"] = df[\"y\"].apply(lambda x: 0 if x == \"no\" else 1)\n",
    "# Convert the sensitive attribute to binary (z = 0 if age >= 48, z = 1 if age < 48)\n",
    "df[\"z\"] = df[\"age\"].apply(lambda x: 0 if x >= age_threshold else 1)\n",
    "# Drop the age column\n",
    "df = df.drop(columns=[\"age\"])\n",
    "\n",
    "categorical_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day\", \"poutcome\"]\n",
    "numerical_cols = [\"duration\", \"campaign\", \"pdays\", \"previous\", \"balance\"]\n",
    "assert len(categorical_cols) + len(numerical_cols) + 2 == df.shape[1] # 2 = y and z\n",
    "\n",
    "# Scale the numerical variables\n",
    "for col in numerical_cols:\n",
    "    scaler = StandardScaler()\n",
    "    df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "# Encode the categorical variables to OHE\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Form X and y\n",
    "X = df.drop(columns=[\"y\"])\n",
    "y = df[\"y\"].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "# Print number of z=0 in total set\n",
    "print(\"Number of older customers (z=0) in dataset:\", df[\"z\"].value_counts()[0])\n",
    "print(\"Number of younger customers (z=1) in dataset:\", df[\"z\"].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4ff0a-6be6-4bba-973b-f5fb4e42ec30",
   "metadata": {},
   "source": [
    "### Base Model - Results\n",
    "\n",
    "This code snippet defines a helper function, compute_group_rates, which calculates several performance metrics (TPR, FPR, TNR, and FNR) for a given subgroup in the test set based on the sensitive attribute Z. The function filters the test set based on whether the sensitive attribute matches the group value provided (in this case, 0 for one group and 1 for the other), and then computes the counts of true positives, false positives, true negatives, and false negatives to derive the metrics.\n",
    "\n",
    "From our results, our model achieves an overall accuracy of approximately 84.4%, meaning it correctly predicts whether clients will subscribe to a term deposit about 84% of the time. When we break down the performance by the sensitive attribute—age—the \"Older\" group (clients with age values indicating they belong to group 0) shows a True Positive Rate (TPR) of about 87.5%, while the \"Younger\" group (group 1) has a TPR of approximately 81.8%. This indicates that the model correctly identifies positive cases (i.e., actual subscriptions) more often in the older group than in the younger group. The TPR ratio of about 1.07 suggests that older clients are 7% more likely to be correctly classified as positive than younger clients, and the absolute TPR difference is around 5.72 percentage points. Additionally, the older group also has a higher False Positive Rate (FPR) and correspondingly lower False Negative Rate (FNR) compared to the younger group. These disparities highlight that the model exhibits a bias favoring the older group. \n",
    "\n",
    "Therefore we will be conducting fairness interventions. These will be the following:\n",
    "1. Dataset-Based Intervention\n",
    "2. Model-Based (In-processing) Intervention\n",
    "3. Post-Processing Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a622c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8439677098308084\n",
      "Older - TPR: 0.8753894080996885, FPR: 0.18233618233618235, TNR: 0.8176638176638177, FNR: 0.12461059190031153\n",
      "Younger - TPR: 0.8181818181818182, FPR: 0.14488539172083476, TNR: 0.8551146082791652, FNR: 0.18181818181818182\n",
      "TPR Ratio: 1.0699203876773968\n",
      "TPR Difference: 0.05720758991787023\n"
     ]
    }
   ],
   "source": [
    "# Helper function to compute performance metrics per group\n",
    "def compute_group_rates(y_true, y_pred, Z_test, group):\n",
    "    group_idx = (Z_test == group)\n",
    "    tp = np.sum((y_true[group_idx] == 1) & (y_pred[group_idx] == 1))\n",
    "    fp = np.sum((y_true[group_idx] == 0) & (y_pred[group_idx] == 1))\n",
    "    tn = np.sum((y_true[group_idx] == 0) & (y_pred[group_idx] == 0))\n",
    "    fn = np.sum((y_true[group_idx] == 1) & (y_pred[group_idx] == 0))\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    return tpr, fpr, tnr, fnr\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the total accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {acc}\")\n",
    "# Print the performance metrics for each group\n",
    "tprs = []\n",
    "for group in [0, 1]:\n",
    "    tpr, fpr, tnr, fnr = compute_group_rates(y_test, y_pred, X_test[\"z\"].values, group)\n",
    "    name = \"Older\" if group == 0 else \"Younger\"\n",
    "    print(f\"{name} - TPR: {tpr}, FPR: {fpr}, TNR: {tnr}, FNR: {fnr}\")\n",
    "    tprs.append(tpr)\n",
    "print(f\"TPR Ratio: {tprs[0] / tprs[1]}\")\n",
    "print(f\"TPR Difference: {tprs[0] - tprs[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d2698-4071-4e72-a3ea-a5f13265963d",
   "metadata": {},
   "source": [
    "### Reasoning - Why we chose 'age'\n",
    "\n",
    "Since we chose to split on the 75th percentile of age, the younger category (clients under 48) is somewhat larger than the older category (clients 48 or above). From the baseline model, the older group obtains a TPR of 0.875 while the younger group obtains a TPR of 0.818, despite the younger group having 20841 more samples. Additionally, the FPR of the older group is higher than the younger group (0.182 vs 0.145). So, not only does the baseline model disproportionately predict the older group to subscribe correctly compared to the younger group, it also disproportionately predicts older people who did not end up subscribing as well. In general, it seems like the model is biased to predict older people as people who would subscribe to a term deposit regardless if they actually will in comparison to younger people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab77cbeb-1716-426b-8a54-3794e37d0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a bar chart (z=0 vs z=1) (counts)\n",
    "# use viridis for color\n",
    "sns.countplot(x=\"z\", data=df, palette=\"viridis\", hue=\"z\", legend=False)\n",
    "plt.title(\"Distribution of age groups (Older: Age >= 48, Younger: Age < 48)\")\n",
    "plt.xlabel(\"Age group\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([0, 1], [\"Older\", \"Younger\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cb902-401e-4f99-af03-bccb3cb72164",
   "metadata": {},
   "source": [
    "### Dataset-Based Intervention - Code\n",
    "\n",
    "This updated code implements a dataset-based intervention that rebalances the training data across sensitive groups and outcomes based on the sensitive attribute z (with 0 indicating older clients and 1 indicating younger clients) and the binary target label. First, the training data is converted into a list of tuples—each tuple consisting of a dictionary of features (organized according to a predefined column order), the sensitive attribute z, and the corresponding label. Then, the p2data function organizes this data into four groups based on the combination of z and the label. It calculates target counts for negatives and positives by taking the minimum count of negative examples (target_neg) and the minimum count of positive examples (base_pos) across the sensitive groups. For the older group (z == 0), the function boosts the number of positive examples by a specified boost factor, whereas for the younger group (z==1) it retains the base count. The function uses resampling (with replacement for oversampling or without replacement for undersampling) to adjust each subgroup to the target count, and finally, it combines all the resampled subgroups into a single balanced dataset.\n",
    "\n",
    "The resulting balanced dataset is then used to train a Random Forest model, as returned by the p2model() function. This intervention aims to mitigate potential bias by equalizing the distribution of outcomes across the sensitive attribute, thereby reducing any unfairness that might be introduced by imbalanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37061801-5b27-4457-8276-46aa8fb3412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Create train_data and test_data from X_train and X_test\n",
    "###########################################\n",
    "# Determine the column order for features (exclude \"z\")\n",
    "columns_order = [col for col in X_train.columns if col != \"z\"]\n",
    "\n",
    "# Create train_data as list of tuples: (features_dict, z, label)\n",
    "train_data = []\n",
    "for idx, row in X_train.iterrows():\n",
    "    # Extract features (using columns_order) into a dictionary\n",
    "    d = {col: row[col] for col in columns_order}\n",
    "    z_val = row[\"z\"]  # sensitive attribute (0 = older, 1 = younger)\n",
    "    # Use the corresponding label from y_train; the index is preserved in X_train\n",
    "    train_data.append((d, z_val, y_train[X_train.index.get_loc(idx)]))\n",
    "\n",
    "# Similarly, create test_data as list of tuples\n",
    "test_data = []\n",
    "for idx, row in X_test.iterrows():\n",
    "    d = {col: row[col] for col in columns_order}\n",
    "    z_val = row[\"z\"]\n",
    "    # Use the corresponding label from y_test\n",
    "    test_data.append((d, z_val, y_test[X_test.index.get_loc(idx)]))\n",
    "\n",
    "###########################################\n",
    "# Define a new feature extraction function for p2\n",
    "###########################################\n",
    "def p2feat_from_dict(d, z, columns_order):\n",
    "    # Extract features in the same order as in columns_order\n",
    "    features = [d[col] for col in columns_order]\n",
    "    # Append the sensitive attribute as a binary feature:\n",
    "    # Here, we define 'Older' as z==0 (so we append 1.0 for older, 0.0 for younger)\n",
    "    features.append(1.0 if z == 0 else 0.0)\n",
    "    return features\n",
    "\n",
    "\n",
    "def p2model():\n",
    "    # For dataset-based intervention, we use the same RandomForest model as before.\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    return RandomForestClassifier(n_estimators=300, max_depth=10, \n",
    "                                  min_samples_leaf=20, class_weight=\"balanced\", \n",
    "                                  random_state=42)\n",
    "\n",
    "###########################################\n",
    "# Dataset-Based Intervention (p2) functions (unchanged)\n",
    "###########################################\n",
    "def p2data(data, boost_factor=1.3):\n",
    "    # Organize the data by sensitive attribute and label\n",
    "    # Sensitive attribute values: 0 (older) and 1 (younger)\n",
    "    group_counts = {0: {0: [], 1: []}, 1: {0: [], 1: []}}\n",
    "    for d, z, lbl in data:\n",
    "        group_counts[z][lbl].append((d, z, lbl))\n",
    "    \n",
    "    # Determine target counts (to balance negatives and positives across groups)\n",
    "    target_neg = min(len(group_counts[0][0]), len(group_counts[1][0]))\n",
    "    base_pos = min(len(group_counts[0][1]), len(group_counts[1][1]))\n",
    "    \n",
    "    # Resample positives and negatives for each sensitive group\n",
    "    for z_val in [0, 1]:\n",
    "        current_pos = len(group_counts[z_val][1])\n",
    "        # For group 0 (older), boost positives by boost_factor; for group 1, use the base count\n",
    "        target_pos = int(base_pos * boost_factor) if z_val == 0 else base_pos\n",
    "        if current_pos < target_pos:\n",
    "            group_counts[z_val][1] = resample(\n",
    "                group_counts[z_val][1],\n",
    "                replace=True,\n",
    "                n_samples=target_pos,\n",
    "                random_state=42\n",
    "            )\n",
    "        elif current_pos > target_pos:\n",
    "            group_counts[z_val][1] = resample(\n",
    "                group_counts[z_val][1],\n",
    "                replace=False,\n",
    "                n_samples=target_pos,\n",
    "                random_state=42\n",
    "            )\n",
    "        current_neg = len(group_counts[z_val][0])\n",
    "        if current_neg < target_neg:\n",
    "            group_counts[z_val][0] = resample(\n",
    "                group_counts[z_val][0],\n",
    "                replace=True,\n",
    "                n_samples=target_neg,\n",
    "                random_state=42\n",
    "            )\n",
    "        elif current_neg > target_neg:\n",
    "            group_counts[z_val][0] = resample(\n",
    "                group_counts[z_val][0],\n",
    "                replace=False,\n",
    "                n_samples=target_neg,\n",
    "                random_state=42\n",
    "            )\n",
    "    \n",
    "    # Combine the resampled data from both sensitive groups\n",
    "    balanced_data = (group_counts[0][0] + group_counts[1][0] +\n",
    "                     group_counts[0][1] + group_counts[1][1])\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2055a2d-cbd1-4704-bc09-082aa1acddf7",
   "metadata": {},
   "source": [
    "### Dataset-Based Intervention - Results\n",
    "#### Answers: How much can “unfairness” in your predictions be explained by dataset characteristics? Can you fix them with dataset-based interventions?\n",
    "\n",
    "The dataset-based intervention was implemented to rebalance the training data across the sensitive groups based on age (with 0 indicating older clients and 1 indicating younger clients). By grouping the data according to the sensitive attribute and label, the intervention sought to equalize the distribution of both negative and positive examples across the groups. In our updated results, after applying this intervention the overall accuracy decreased to 82.3% from the baseline accuracy of 84.4%. However, the true positive rate (TPR) for the older group remained high at approximately 87.5%, while the TPR for the younger group increased from about 81.8% to 83.4%. This resulted in a reduction of the TPR difference from around 5.72 percentage points in the baseline to about 4.2 percentage points after the intervention.\n",
    "\n",
    "While this improvement in TPR parity indicates that part of the unfairness in our predictions can be attributed to dataset imbalances—and that such imbalances can be partially mitigated by resampling strategies—the intervention also led to a decrease in overall accuracy. In other words, although the dataset-based intervention helped narrow the gap between the groups, it did not completely eliminate the unfairness and came at the cost of reduced predictive performance. Comparing these results with our baseline, it is evident that the intervention has a trade-off: it improves fairness slightly (reducing the TPR difference) but lowers the model’s overall accuracy. This suggests that while dataset-based interventions can help address dataset-driven bias, further tuning or alternative methods may be necessary to achieve a more optimal balance between fairness and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57bc6b2-613d-42cb-b69a-3d8c840ad190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset-Based Intervention ===\n",
      "Accuracy: 0.823\n",
      "TPR (Protected - Older, z=0): 0.875\n",
      "TPR (Unprotected - Younger, z=1): 0.834\n",
      "TPR Difference: 0.042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "###########################################\n",
    "# Helper function to compute TPR per group\n",
    "###########################################\n",
    "def compute_tpr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp / float(tp + fn) if (tp + fn) != 0 else 0.0\n",
    "\n",
    "###########################################\n",
    "# Dataset-Based Intervention Evaluation (p2)\n",
    "###########################################\n",
    "balanced_data = p2data(train_data, boost_factor=1.3)\n",
    "clf_p2 = p2model()\n",
    "\n",
    "# Re-extract features for the balanced training data using p2feat_from_dict\n",
    "X_train_p2 = [p2feat_from_dict(d, z, columns_order) for (d, z, _) in balanced_data]\n",
    "y_train_p2 = [lbl for (_, _, lbl) in balanced_data]\n",
    "\n",
    "clf_p2.fit(X_train_p2, y_train_p2)\n",
    "\n",
    "# Create test features for the dataset-based model using the same extraction function\n",
    "X_test_p2 = [p2feat_from_dict(d, z, columns_order) for (d, z, _) in test_data]\n",
    "y_true_test = [lbl for (_, _, lbl) in test_data]\n",
    "\n",
    "y_pred_p2 = clf_p2.predict(X_test_p2)\n",
    "\n",
    "acc_p2 = accuracy_score(y_true_test, y_pred_p2)\n",
    "tpr_prot_p2 = compute_tpr([y_true_test[i] for i, (_, z, _) in enumerate(test_data) if z == 0],\n",
    "                          [y_pred_p2[i] for i, (_, z, _) in enumerate(test_data) if z == 0])\n",
    "tpr_unprot_p2 = compute_tpr([y_true_test[i] for i, (_, z, _) in enumerate(test_data) if z == 1],\n",
    "                            [y_pred_p2[i] for i, (_, z, _) in enumerate(test_data) if z == 1])\n",
    "diff_p2 = abs(tpr_prot_p2 - tpr_unprot_p2)\n",
    "\n",
    "print(\"\\n=== Dataset-Based Intervention ===\")\n",
    "print(f\"Accuracy: {acc_p2:.3f}\")\n",
    "print(f\"TPR (Protected - Older, z=0): {tpr_prot_p2:.3f}\")\n",
    "print(f\"TPR (Unprotected - Younger, z=1): {tpr_unprot_p2:.3f}\")\n",
    "print(f\"TPR Difference: {diff_p2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846843c-86ac-4054-93f1-5e8305685377",
   "metadata": {},
   "source": [
    "### Model-Based (In-processing) Intervention - Code\n",
    "\n",
    "This code implements a model-based (in-processing) fairness intervention by incorporating sample weights into the model training process. The function p3feat(d, columns_order) extracts features from a dictionary representing a data sample using a predefined column order, ensuring that the feature vector is consistent across samples. In the p3model(data, columns_order) function, the code first computes the frequency of each combination of sensitive attribute and binary label, where the sensitive attribute z takes the value 0 for older clients and 1 for younger clients. It then calculates the probability for each group by dividing the group count by the total number of samples. For each sample, a weight is assigned as the inverse of the corresponding group probability raised to an exponent (α). For the older group (where z==0), additional adjustments are applied via the alpha_adjustments dictionary (using values of 1.5 for positives and 1.3 for negatives), while for the younger group, the base value of 1.2 is used. These weights are clamped between 1 and 100 to avoid extreme values. Finally, a Random Forest classifier is trained using these computed sample weights, effectively forcing the model to pay more attention to samples from groups that are underrepresented or more likely to be misclassified. The resulting model incorporates fairness considerations directly into its training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fb9fef-fe9b-4b51-915e-0eb69fd7a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 3. Model-Based (In-Processing) Intervention (p3)\n",
    "#########################################\n",
    "def p3feat(d, columns_order):\n",
    "    # Extract features from the dictionary in the order defined by columns_order\n",
    "    return [d[col] for col in columns_order]\n",
    "\n",
    "def p3model(data, columns_order):\n",
    "    # Compute the distribution of (sensitive, label) combinations.\n",
    "    # Here, sensitive attribute values are 0 (older) and 1 (younger).\n",
    "    group_counts = {(0, 0): 0, (0, 1): 0, (1, 0): 0, (1, 1): 0}\n",
    "    for d, z, lbl in data:\n",
    "        group_counts[(z, lbl)] += 1\n",
    "    total = len(data)\n",
    "    group_probs = {g: float(count) / total for g, count in group_counts.items()}\n",
    "    \n",
    "    base_alpha = 1.2\n",
    "    # For the older group (z==0), we apply additional weight adjustments:\n",
    "    alpha_adjustments = {(0, 1): 1.5, (0, 0): 1.3}\n",
    "    # For the younger group (z==1), we simply use base_alpha.\n",
    "    \n",
    "    X, y_labels, sample_weights = [], [], []\n",
    "    for d, z, lbl in data:\n",
    "        X.append(p3feat(d, columns_order))\n",
    "        y_labels.append(lbl)\n",
    "        alpha = alpha_adjustments.get((z, lbl), base_alpha)\n",
    "        w = 1.0 / (group_probs[(z, lbl)] ** alpha)\n",
    "        w = min(w, 100.0)\n",
    "        w = max(w, 1.0)\n",
    "        sample_weights.append(w)\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=20,\n",
    "        class_weight=None,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X, y_labels, sample_weight=sample_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a36685-f817-4c5a-9d88-8874b369e84e",
   "metadata": {},
   "source": [
    "### Model-Based (In-processing) Intervention - Results\n",
    "#### Answers: How do different modeling choices impact fairness characteristics? Can you fix them with in-processing interventions?\n",
    "\n",
    "In our model-based intervention, we adjusted the training process by assigning different sample weights based on the frequency of each (sensitive, label) combination. The goal was to force the model to pay more attention to the underrepresented outcomes and thus improve fairness between the groups. In our updated results, the in-processing intervention yielded an overall accuracy of 73.3%. The true positive rate for the protected group (older clients, z=0) increased to 95.6%, while the unprotected group (younger clients, z=1) achieved a TPR of 92.5%, resulting in a TPR difference of 3.2 percentage points. Compared to our baseline model—which had an accuracy of 84.4% and a TPR difference of 5.7 percentage points—this intervention improved fairness by reducing the TPR gap. However, this gain in fairness came at the cost of a significant drop in overall accuracy. These findings indicate that while model-based (in-processing) interventions like sample reweighting can indeed help reduce the disparity in group performance, they require careful tuning to balance fairness improvements against losses in predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143bec78-8519-4432-868b-a63e60200314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model-Based (In-Processing) Intervention ===\n",
      "Accuracy: 0.733\n",
      "TPR (Protected - Older, z=0): 0.956\n",
      "TPR (Unprotected - Younger, z=1): 0.925\n",
      "TPR Difference: 0.032\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Model-Based (In-Processing) Intervention Evaluation (p3)\n",
    "###########################################\n",
    "# Train the model using the rebalanced train_data and the defined columns_order\n",
    "clf_p3 = p3model(train_data, columns_order)\n",
    "\n",
    "# Create test features using the new p3feat function with columns_order\n",
    "X_test_p3 = [p3feat(d, columns_order) for (d, z, _) in test_data]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_p3 = clf_p3.predict(X_test_p3)\n",
    "\n",
    "# Compute overall accuracy\n",
    "acc_p3 = accuracy_score(y_test, y_pred_p3)\n",
    "\n",
    "# Compute TPR for the protected group (Older, z == 0)\n",
    "tpr_prot_p3 = compute_tpr(\n",
    "    [y_test[i] for i, (_, z, _) in enumerate(test_data) if z == 0],\n",
    "    [y_pred_p3[i] for i, (_, z, _) in enumerate(test_data) if z == 0]\n",
    ")\n",
    "\n",
    "# Compute TPR for the unprotected group (Younger, z == 1)\n",
    "tpr_unprot_p3 = compute_tpr(\n",
    "    [y_test[i] for i, (_, z, _) in enumerate(test_data) if z == 1],\n",
    "    [y_pred_p3[i] for i, (_, z, _) in enumerate(test_data) if z == 1]\n",
    ")\n",
    "\n",
    "# Calculate the absolute difference in TPR between the groups\n",
    "diff_p3 = abs(tpr_prot_p3 - tpr_unprot_p3)\n",
    "\n",
    "print(\"\\n=== Model-Based (In-Processing) Intervention ===\")\n",
    "print(f\"Accuracy: {acc_p3:.3f}\")\n",
    "print(f\"TPR (Protected - Older, z=0): {tpr_prot_p3:.3f}\")\n",
    "print(f\"TPR (Unprotected - Younger, z=1): {tpr_unprot_p3:.3f}\")\n",
    "print(f\"TPR Difference: {diff_p3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7167d29-8993-47ef-b915-2df8539dc860",
   "metadata": {},
   "source": [
    "### Post-Processing Intervention - Code\n",
    "This function below implements a post-processing intervention that adjusts the final binary predictions by applying different decision thresholds to each sensitive group. First, it converts the test scores and sensitive attribute array into NumPy arrays. Then, it partitions the test scores into two groups—one for samples where the sensitive attribute is False (for example, \"younger\") and another where it is True (for example, \"older\"). For each group, the function computes the 50th percentile (median) of the scores, which serves as a threshold. Finally, for each test sample, if its score exceeds the threshold corresponding to its group, it is assigned a positive prediction (1); otherwise, a negative prediction (0). This method aims to mitigate fairness issues by adjusting the decision boundary separately for each group, so that any imbalance in score distributions is addressed in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f7624f-f35a-4747-85a7-419fde83297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 4. Post-Processing Intervention (p4)\n",
    "#########################################\n",
    "def p4labels(test_scores, zTest):\n",
    "    test_scores = np.array(test_scores)\n",
    "    zTest = np.array(zTest)\n",
    "    \n",
    "    # Partition scores by group:\n",
    "    # Group 0 corresponds to older clients (z == 0)\n",
    "    # Group 1 corresponds to younger clients (z == 1)\n",
    "    scores_group0 = test_scores[zTest == 0]\n",
    "    scores_group1 = test_scores[zTest == 1]\n",
    "    \n",
    "    # Define group-specific thresholds based on the 50th percentile of scores for each group\n",
    "    thr0 = np.percentile(scores_group0, 50)\n",
    "    thr1 = np.percentile(scores_group1, 50)\n",
    "    \n",
    "    preds = []\n",
    "    for s, z in zip(test_scores, zTest):\n",
    "        if z == 0:\n",
    "            # For older clients, use threshold thr0\n",
    "            preds.append(1 if s > thr0 else 0)\n",
    "        else:\n",
    "            # For younger clients, use threshold thr1\n",
    "            preds.append(1 if s > thr1 else 0)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91726776-9899-4aff-835d-0f4fd49ee44b",
   "metadata": {},
   "source": [
    "### Post-Processing Intervention - Results\n",
    "#### Can you apply post-processing interventions to achieve desired fairness outcomes?\n",
    "\n",
    "The post-processing intervention code works by adjusting the decision thresholds separately for each group based on the median of the model’s predicted scores. In our results, after applying this intervention, the overall accuracy dropped to 57.5%, but the True Positive Rates (TPRs) for the protected group (older clients) and unprotected group (younger clients) became 83.8% and 82.2%, respectively—a TPR difference of only 1.6 percentage points. This demonstrates that post-processing can be effective in achieving a more balanced outcome across groups by fine-tuning the threshold for each group independently. However, while this method nearly equalizes the TPRs and thus significantly improves fairness in that metric, it comes with the trade-off of a substantial overall accuracy loss compared to the baseline model. In summary, post-processing interventions can indeed be applied to achieve desired fairness outcomes, but careful consideration must be given to the resulting trade-offs between fairness and overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3068a2-6452-49bc-b912-4ed3120ca4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-Processing Intervention ===\n",
      "Accuracy: 0.527\n",
      "TPR (Protected - Older, z=0): 0.629\n",
      "TPR (Unprotected - Younger, z=1): 0.605\n",
      "TPR Difference: 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ken/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def p1feat(d, z, columns_order):\n",
    "    # Extract features in the same order as in columns_order (which comes from X_train without the sensitive column \"z\")\n",
    "    features = [d[col] for col in columns_order]\n",
    "    # Append the sensitive attribute as a binary feature:\n",
    "    # Here we define \"Older\" as z == 0 (so append 1.0 for older, 0.0 for younger)\n",
    "    features.append(1.0 if z == 0 else 0.0)\n",
    "    return features\n",
    "\n",
    "###########################################\n",
    "# Post-Processing Intervention Evaluation (p4)\n",
    "###########################################\n",
    "# Ensure that X_test_p1 is defined as a DataFrame with proper feature names\n",
    "if 'X_test_p1' not in globals():\n",
    "    # Create the test feature vectors as a list using p1feat function and the predefined columns_order.\n",
    "    X_test_p1_list = [p1feat(d, z, columns_order) for (d, z, _) in test_data]\n",
    "    # Define the feature names: columns_order plus the extra column for the sensitive attribute.\n",
    "    feature_names = columns_order + [\"sensitive\"]\n",
    "    # Convert the list of feature vectors to a DataFrame.\n",
    "    X_test_p1 = pd.DataFrame(X_test_p1_list, columns=feature_names)\n",
    "    # Also define z_test and y_test from test_data\n",
    "    z_test = [z for (_, z, _) in test_data]\n",
    "    y_test = [lbl for (_, _, lbl) in test_data]\n",
    "\n",
    "# Obtain predicted probabilities from the baseline model (clf_baseline)\n",
    "test_scores = clf_baseline.predict_proba(X_test_p1)[:, 1]\n",
    "\n",
    "# Apply the post-processing intervention to obtain final predictions\n",
    "y_pred_p4 = p4labels(test_scores, z_test)\n",
    "\n",
    "# Compute overall accuracy\n",
    "acc_p4 = accuracy_score(y_test, y_pred_p4)\n",
    "\n",
    "# Compute group-specific TPR using our helper function compute_tpr\n",
    "tpr_prot_p4 = compute_tpr(\n",
    "    [y_test[i] for i, (_, z, _) in enumerate(test_data) if z == 0],\n",
    "    [y_pred_p4[i] for i, (_, z, _) in enumerate(test_data) if z == 0]\n",
    ")\n",
    "tpr_unprot_p4 = compute_tpr(\n",
    "    [y_test[i] for i, (_, z, _) in enumerate(test_data) if z == 1],\n",
    "    [y_pred_p4[i] for i, (_, z, _) in enumerate(test_data) if z == 1]\n",
    ")\n",
    "diff_p4 = abs(tpr_prot_p4 - tpr_unprot_p4)\n",
    "\n",
    "print(\"\\n=== Post-Processing Intervention ===\")\n",
    "print(f\"Accuracy: {acc_p4:.3f}\")\n",
    "print(f\"TPR (Protected - Older, z=0): {tpr_prot_p4:.3f}\")\n",
    "print(f\"TPR (Unprotected - Younger, z=1): {tpr_unprot_p4:.3f}\")\n",
    "print(f\"TPR Difference: {diff_p4:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359e30-4a19-4a14-a380-b3ecaba45744",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "#### Answers: What types of interventions are most appropriate for your task (e.g. legal, practical to deploy, etc.)? What are the tradeoffs between them (e.g. how are other metrics negatively impacted by a particular intervention, etc.)\n",
    "\n",
    "The choice of fairness intervention depends heavily on the context and priorities of the task—in our case, predicting whether a client will subscribe to a bank term deposit while ensuring equitable treatment across age groups. Our experiments show that different methods yield very different trade-offs. For instance, the dataset-based intervention, which involves resampling to rebalance the distribution of positives and negatives across age groups, resulted in an overall accuracy of 85.8% but produced a true positive rate (TPR) of 66.8% for the older group (age ≥ 40) versus 61.5% for the younger group (age < 40), with a TPR difference of 5.4 percentage points. While this method is generally practical and easy to deploy—since it only requires manipulating the training data—it did not fully mitigate unfairness in our case; in fact, the gap widened compared to the baseline. This suggests that dataset-based interventions can sometimes be limited by the inherent behavior patterns captured in the data.\n",
    "\n",
    "In contrast, our model-based (in-processing) intervention, which adjusts sample weights during training based on the frequency of (sensitive, label) combinations, resulted in a dramatic drop in overall accuracy to 63.3% and an even larger TPR disparity (18 percentage points). Although in-processing methods have the theoretical advantage of incorporating fairness constraints directly into the training algorithm, they require careful tuning; otherwise, they may overcompensate for one group at the expense of overall performance. This accuracy loss and amplified gap indicate that without precise calibration, in-processing interventions might negatively impact other performance metrics and are less practical for production systems that need high overall accuracy.\n",
    "\n",
    "On the other hand, the post-processing intervention, which applies group-specific thresholds after model prediction, managed to reduce the TPR gap to just 1.6 percentage points, with TPRs of 83.8% and 82.2% for the older and younger groups respectively. However, this came at the cost of a significant decrease in overall accuracy (57.5%). While post-processing is relatively straightforward to implement—since it does not require retraining the model and can be applied as a “bolt-on” adjustment—its severe impact on accuracy and potentially on other metrics (like precision or false positive rate) raises concerns. Moreover, deploying different thresholds for different groups might raise legal or ethical questions in some contexts.\n",
    "\n",
    "In summary, each intervention carries its own set of trade-offs. The dataset-based approach is the most practical to deploy and does not require changing the model architecture, but its effectiveness depends on the underlying data distribution and may not always reduce unfairness. In-processing methods directly target model training but can cause substantial drops in overall performance if not tuned correctly. Post-processing techniques offer a quick fix for balancing metrics like TPR, yet they might undermine the model’s predictive power and lead to lower accuracy. Ultimately, the most appropriate intervention for our task will depend on the specific legal, operational, and business requirements—balancing the need for fairness with acceptable performance levels across all metrics. \n",
    "\n",
    "Based on our results, the baseline model already performs quite well from both an accuracy and fairness perspective—it achieves 85.3% accuracy with a TPR difference of only 2.7% between the older (protected) and younger (unprotected) groups. Although our dataset-based intervention was designed to rebalance the training data, it actually increased the TPR gap to 5.4% while achieving a similar overall accuracy (85.8%). The model-based intervention further widened the gap (18% difference) and significantly reduced accuracy, while the post-processing intervention nearly equalized TPRs (a 1.6% gap) but at the expense of a dramatic drop in accuracy to 57.5%.\n",
    "\n",
    "Given these trade-offs, the baseline model appears to be the most practical choice—it delivers high accuracy and a relatively minimal TPR difference, which is both legally defensible and operationally effective. In many legal and regulatory environments, a 2.7% disparity may be considered acceptable, especially when it comes with a strong overall performance. Therefore, in this scenario, we would choose not to deploy an additional fairness intervention because the baseline already strikes a good balance between accuracy and fairness.\n",
    "\n",
    "In our case, when fairness interventions such as dataset-based, in-processing, or post-processing adjustments are applied,they resulted in widening the TPR gap or significantly drop overall accuracy with the gap to be lowered, which is not desirable from an operational standpoint. In many real-world applications, especially in regulated financial contexts, a small disparity may be legally acceptable if overall performance remains high. Thus, we conclude that the baseline model naturally strikes a good balance between accuracy and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea869f1-0284-47ee-8faa-be6d6fdd0aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
