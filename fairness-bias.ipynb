{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca64366-3cb3-472c-9bb7-d3a1cd9f62c6",
   "metadata": {},
   "source": [
    "### Sensitive Attribute (z) is age.\n",
    "\n",
    "We chose age as our sensitive attribute because age it often correlates with differing financial behaviors and needs. For example, younger individuals typically require more liquidity and are less likely to commit to long-term savings products like bank term deposits due to their dynamic financial circumstances and greater likelihood of needing immediate access to cash. On the other hand, older individuals often have more financial stability and may be more inclined to lock in funds for a higher return. Additionally, many of the other features such as marital status, job type, if the customer has a housing loan or not, and much more are correlated to age.\n",
    "\n",
    "This naturally creates a potential for unfairness where models trained on historical data might inadvertently favor older clients while neglecting younger ones—even if the underlying behavior differences are statistically true on average. By focusing on age, we can analyze and quantify these disparities (e.g., through differences in true positive rates) and then apply fairness interventions to ensure that the model does not systematically disadvantage younger clients, making our results both interesting and practically relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee26f4c-072f-406b-9751-25aa10e7e896",
   "metadata": {},
   "source": [
    "### Fairness Metric - Equal Opportunity\n",
    "We chose equal opportunity as our primary fairness metric because it directly measures the model's ability to correctly identify positive cases (i.e., clients who subscribe to a term deposit) across different groups. From the perspective of the bank, we want to get as many customers to subscribe to a term deposit as possible. If a person would be interested to a term deposit, we want to ensure that we speak to them to secure the deposit. In contrast, it's OK if this comes a little bit at a cost of an increase in failed calls. In terms of our model, we want to increase the true positive rate (TPR) of the underserved group to match the better-served group. Ideally, we achieve this with minimal loss in accuracy (to prevent wasted time or marketing efforts).\n",
    "\n",
    "In our context, ensuring that the TPR is as close as possible for both the better-served group (e.g., older clients) and the underserved group (younger clients) is critical for guaranteeing equal opportunity. A low TPR difference indicates that clients with a genuine likelihood to subscribe are equally recognized by the model, regardless of their age. This metric is particularly relevant because misclassifying a true positive (especially for a group that might already be under-targeted) can lead to missed opportunities and potential discrimination. Using a ratio of the TPRs allows us to quantify fairness in a way that addresses the core objective of our predictive task.\n",
    "\n",
    "Misclassifying a client who is likely to subscribe (a false negative) can mean denying a beneficial financial product to that individual, which is especially problematic if it happens disproportionately for one age group. By focusing on the ratio of the TPRs, we ensure that both older (protected) and younger (unprotected) clients who are truly eligible for the product are equally likely to be identified by the model, which maximizes the effects of the marketing campaign. Our argument emphasizes that, given the potential consequences for customer access and fairness in financial services, using the ratio of TPRs or equal opportunity as our fairness metric is not only justified but essential for the responsible deployment of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733eec9c-0572-4b34-9ac1-c43a48c3fcdd",
   "metadata": {},
   "source": [
    "### Data Preparation + Base Model\n",
    "\n",
    "We import the necessary libraries (pandas, NumPy, scikit-learn, etc.) and load our data `bank-full.csv`. For data pre-processing, we simply just converted all categorical data into one-hot-encodings and scaled numerical values for faster convergence. We then converts the label to binary indicating whether a client subscribed to a term deposit or not and then created the sensitive attribute z based on age. To create a binary variable and separate the \"older\" group from the \"younger\" group, we split on the 75th percentile ($z=0$ if $\\text{age} \\geq 48$, $z=1$ otherwise) The data is split into training and test sets. \n",
    "\n",
    "We used a logstic regression model as our baseline and define a helper function for calculating the True Positive Rate (TPR) to measure equal opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "945d1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age threshold: 48.0\n",
      "Train set size: 36168\n",
      "Test set size: 9043\n",
      "Number of older customers (z=0) in dataset: 12185\n",
      "Number of younger customers (z=1) in dataset: 33026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data_path = \"data/bank-full.csv\"\n",
    "df_original = pd.read_csv(data_path, sep=\";\")\n",
    "df = df_original.copy()\n",
    "\n",
    "# Obtain the 75th percentile of the age column to use as the threshold for the sensitive attribute\n",
    "age_threshold = df[\"age\"].quantile(0.75)\n",
    "print(\"Age threshold:\", age_threshold)\n",
    "\n",
    "# Convert the target variable to binary (0 = No, 1 = Yes)\n",
    "df[\"y\"] = df[\"y\"].apply(lambda x: 0 if x == \"no\" else 1)\n",
    "# Convert the sensitive attribute to binary (z = 0 if age >= 45, z = 1 if age < 45)\n",
    "df[\"z\"] = df[\"age\"].apply(lambda x: 0 if x >= age_threshold else 1)\n",
    "# Drop the age column\n",
    "df = df.drop(columns=[\"age\"])\n",
    "\n",
    "categorical_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day\", \"poutcome\"]\n",
    "numerical_cols = [\"duration\", \"campaign\", \"pdays\", \"previous\", \"balance\"]\n",
    "assert len(categorical_cols) + len(numerical_cols) + 2 == df.shape[1] # 2 = y and z\n",
    "\n",
    "# Scale the numerical variables\n",
    "for col in numerical_cols:\n",
    "    scaler = StandardScaler()\n",
    "    df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "# Encode the categorical variables to OHE\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Form X and y\n",
    "X = df.drop(columns=[\"y\"])\n",
    "y = df[\"y\"].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "# Print number of z=0 in total set\n",
    "print(\"Number of older customers (z=0) in dataset:\", df[\"z\"].value_counts()[0])\n",
    "print(\"Number of younger customers (z=1) in dataset:\", df[\"z\"].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a622c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8439677098308084\n",
      "Older - TPR: 0.8753894080996885, FPR: 0.18233618233618235, TNR: 0.8176638176638177, FNR: 0.12461059190031153\n",
      "Younger - TPR: 0.8181818181818182, FPR: 0.14488539172083476, TNR: 0.8551146082791652, FNR: 0.18181818181818182\n",
      "TPR Ratio: 1.0699203876773968\n",
      "TPR Difference: 0.05720758991787023\n"
     ]
    }
   ],
   "source": [
    "# Helper function to compute performance metrics per group\n",
    "def compute_group_rates(y_true, y_pred, Z_test, group):\n",
    "    group_idx = (Z_test == group)\n",
    "    tp = np.sum((y_true[group_idx] == 1) & (y_pred[group_idx] == 1))\n",
    "    fp = np.sum((y_true[group_idx] == 0) & (y_pred[group_idx] == 1))\n",
    "    tn = np.sum((y_true[group_idx] == 0) & (y_pred[group_idx] == 0))\n",
    "    fn = np.sum((y_true[group_idx] == 1) & (y_pred[group_idx] == 0))\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    return tpr, fpr, tnr, fnr\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(C=1, max_iter=1000, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the total accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {acc}\")\n",
    "# Print the performance metrics for each group\n",
    "tprs = []\n",
    "for group in [0, 1]:\n",
    "    tpr, fpr, tnr, fnr = compute_group_rates(y_test, y_pred, X_test[\"z\"].values, group)\n",
    "    name = \"Older\" if group == 0 else \"Younger\"\n",
    "    print(f\"{name} - TPR: {tpr}, FPR: {fpr}, TNR: {tnr}, FNR: {fnr}\")\n",
    "    tprs.append(tpr)\n",
    "print(f\"TPR Ratio: {tprs[0] / tprs[1]}\")\n",
    "print(f\"TPR Difference: {tprs[0] - tprs[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d2698-4071-4e72-a3ea-a5f13265963d",
   "metadata": {},
   "source": [
    "### Reasoning - Why we chose 'age'\n",
    "\n",
    "Since we chose to split on the 75th percentile of age, the younger category (clients under 48) is somewhat larger than the older category (clients 48 or above). From the baseline model, the older group obtains a TPR of 0.875 while the younger group obtains a TPR of 0.818, despite the younger group having 20841 more samples. Additionally, the FPR of the older group is higher than the younger group (0.182 vs 0.145). So, not only does the baseline model disproportionately predict the older group to subscribe correctly compared to the younger group, it also disproportionately predicts older people who did not end up subscribing as well. In general, it seems like the model is biased to predict older people as people who would subscribe to a term deposit regardless if they actually will in comparison to younger people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab77cbeb-1716-426b-8a54-3794e37d0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a bar chart (z=0 vs z=1) (counts)\n",
    "# use viridis for color\n",
    "sns.countplot(x=\"z\", data=df, palette=\"viridis\", hue=\"z\", legend=False)\n",
    "plt.title(\"Distribution of age groups (Older: Age >= 48, Younger: Age < 48)\")\n",
    "plt.xlabel(\"Age group\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([0, 1], [\"Older\", \"Younger\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cb902-401e-4f99-af03-bccb3cb72164",
   "metadata": {},
   "source": [
    "### Dataset-Based Intervention - Code\n",
    "\n",
    "This code implements a dataset-based intervention aimed at mitigating potential bias by rebalancing the training data across sensitive groups and outcomes. First, it organizes the data into four groups based on the sensitive attribute z (for example, age-based: True if age ≥ 40, False otherwise) and the binary label (0 or 1). It then determines the target count for each subgroup by taking the minimum counts among negatives and positives across the sensitive groups. For each group, if there are fewer positive or negative examples than the target, the code oversamples those examples using resampling with replacement; if there are too many, it undersamples them without replacement. Finally, the balanced subgroups are combined into one dataset that aims to equalize the distribution of labels across the sensitive attribute. The accompanying p2model() function simply returns the baseline Random Forest model, which will then be trained on this rebalanced data, potentially reducing any unfairness that may have been introduced by imbalanced training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37061801-5b27-4457-8276-46aa8fb3412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 2. Dataset-Based Intervention (p2)\n",
    "#########################################\n",
    "def p2data(data, boost_factor=1.3):\n",
    "    # Organize the data by sensitive attribute and label\n",
    "    group_counts = { True: {0: [], 1: []}, False: {0: [], 1: []} }\n",
    "    for d, z, lbl in data:\n",
    "        group_counts[z][lbl].append((d, z, lbl))\n",
    "    \n",
    "    # Identify target counts to balance negatives and positives\n",
    "    target_neg = min(len(group_counts[True][0]), len(group_counts[False][0]))\n",
    "    base_pos = min(len(group_counts[True][1]), len(group_counts[False][1]))\n",
    "    \n",
    "    # Resample positives and negatives for each group\n",
    "    for z_val in [True, False]:\n",
    "        current_pos = len(group_counts[z_val][1])\n",
    "        target_pos = int(base_pos * boost_factor) if z_val is True else base_pos\n",
    "        if current_pos < target_pos:\n",
    "            group_counts[z_val][1] = resample(\n",
    "                group_counts[z_val][1],\n",
    "                replace=True,\n",
    "                n_samples=target_pos,\n",
    "                random_state=42\n",
    "            )\n",
    "        elif current_pos > target_pos:\n",
    "            group_counts[z_val][1] = resample(\n",
    "                group_counts[z_val][1],\n",
    "                replace=False,\n",
    "                n_samples=target_pos,\n",
    "                random_state=42\n",
    "            )\n",
    "        current_neg = len(group_counts[z_val][0])\n",
    "        if current_neg < target_neg:\n",
    "            group_counts[z_val][0] = resample(\n",
    "                group_counts[z_val][0],\n",
    "                replace=True,\n",
    "                n_samples=target_neg,\n",
    "                random_state=42\n",
    "            )\n",
    "        elif current_neg > target_neg:\n",
    "            group_counts[z_val][0] = resample(\n",
    "                group_counts[z_val][0],\n",
    "                replace=False,\n",
    "                n_samples=target_neg,\n",
    "                random_state=42\n",
    "            )\n",
    "    \n",
    "    # Combine all groups\n",
    "    balanced_data = (group_counts[True][0] + group_counts[False][0] +\n",
    "                     group_counts[True][1] + group_counts[False][1])\n",
    "    return balanced_data\n",
    "\n",
    "def p2model():\n",
    "    # For dataset-based, we can use the same model as the baseline.\n",
    "    return p1model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2055a2d-cbd1-4704-bc09-082aa1acddf7",
   "metadata": {},
   "source": [
    "### Dataset-Based Intervention - Results\n",
    "#### Answers: How much can “unfairness” in your predictions be explained by dataset haracteristics? Can you fix them with dataset-based interventions?\n",
    "\n",
    "The dataset-based intervention was designed to rebalance the training data across the sensitive groups (in this case, based on age) so that the model would have a more equal representation of positive and negative outcomes from both older (protected, age ≥ 40) and younger (unprotected, age < 40) clients. In our results, we observe that the overall accuracy increased slightly to 85.8%. However, the intervention resulted in a True Positive Rate (TPR) of 66.8% for older clients and 61.5% for younger clients, leading to a TPR difference of 5.4 percentage points.\n",
    "\n",
    "While the idea behind dataset-based interventions is to reduce unfairness caused by imbalances in the training data, our results suggest that, in this instance, the intervention actually increased the gap between the groups. This could indicate that the rebalancing process overcorrected for the underrepresented outcomes or that the underlying dataset characteristics (such as the natural differences in subscription behavior between age groups) are too strong to be fully mitigated by a simple resampling strategy. In other words, although part of the unfairness might be explained by dataset characteristics, our current dataset-based intervention did not fix the unfairness—instead, it amplified the disparity in TPR between the older and younger groups. This result suggests that additional tuning or alternative fairness interventions (such as model-based or post-processing methods) might be needed to achieve a more balanced outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57bc6b2-613d-42cb-b69a-3d8c840ad190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset-Based Intervention ===\n",
      "Accuracy: 0.858\n",
      "TPR (Protected): 0.668\n",
      "TPR (Unprotected): 0.615\n",
      "TPR Difference: 0.054\n"
     ]
    }
   ],
   "source": [
    "    ###########################################\n",
    "    # Dataset-Based Intervention Evaluation (p2)\n",
    "    ###########################################\n",
    "    balanced_data = p2data(train_data, boost_factor=1.3)\n",
    "    clf_p2 = p2model()\n",
    "    X_train_p2 = [p1feat(d, z) for (d, z, _) in balanced_data]\n",
    "    y_train_p2 = [lbl for (_, _, lbl) in balanced_data]\n",
    "    clf_p2.fit(X_train_p2, y_train_p2)\n",
    "    y_pred_p2 = clf_p2.predict(X_test_p1)\n",
    "    \n",
    "    acc_p2 = accuracy_score(y_test, y_pred_p2)\n",
    "    tpr_prot_p2 = tpr([y_test[i] for i, z in enumerate(z_test) if z],\n",
    "                      [y_pred_p2[i] for i, z in enumerate(z_test) if z])\n",
    "    tpr_unprot_p2 = tpr([y_test[i] for i, z in enumerate(z_test) if not z],\n",
    "                        [y_pred_p2[i] for i, z in enumerate(z_test) if not z])\n",
    "    diff_p2 = abs(tpr_prot_p2 - tpr_unprot_p2)\n",
    "    \n",
    "    print(\"\\n=== Dataset-Based Intervention ===\")\n",
    "    print(f\"Accuracy: {acc_p2:.3f}\")\n",
    "    print(f\"TPR (Protected): {tpr_prot_p2:.3f}\")\n",
    "    print(f\"TPR (Unprotected): {tpr_unprot_p2:.3f}\")\n",
    "    print(f\"TPR Difference: {diff_p2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846843c-86ac-4054-93f1-5e8305685377",
   "metadata": {},
   "source": [
    "### Model-Based (In-processing) Intervention - Code\n",
    "\n",
    "This code implements a model-based (in-processing) fairness intervention that adjusts the training process by assigning different weights to each sample based on its group frequency. The function p3feat(d) extracts a fixed set of numeric features from a dictionary representing a data sample (including fields like age, campaign, pdays, and so on), converting them into a list of floating-point numbers. In the p3model(data) function, the code first computes the frequency of each combination of sensitive attribute (here, age-based, with the sensitive attribute z being either True or False) and binary label (0 or 1). It then calculates the probability for each group by dividing the group count by the total number of samples. For each sample, the code assigns a weight that is the inverse of the corresponding group probability raised to a power (alpha), where a higher alpha value (taken from the alpha_adjustments dictionary for certain groups) increases the weight for underrepresented groups. These weights are clamped to lie between 1 and 100 to avoid extreme values. Finally, the function trains a Random Forest classifier using these computed sample weights, effectively forcing the model to pay more attention to samples from groups that are underrepresented or more likely to be misclassified. The resulting model is then returned, incorporating fairness considerations directly into its training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fb9fef-fe9b-4b51-915e-0eb69fd7a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 3. Model-Based (In-Processing) Intervention (p3)\n",
    "#########################################\n",
    "def p3feat(d):\n",
    "    numeric_keys = [\n",
    "        'age', \n",
    "        'campaign', \n",
    "        'pdays', \n",
    "        'previous', \n",
    "        'emp.var.rate',\n",
    "        'cons.price.idx',\n",
    "        'cons.conf.idx',\n",
    "        'euribor3m',\n",
    "        'nr.employed'\n",
    "    ]\n",
    "    return [float(d.get(key, 0)) for key in numeric_keys]\n",
    "\n",
    "def p3model(data):\n",
    "    # Compute the distribution of (sensitive, label) combinations\n",
    "    group_counts = { (False, 0): 0, (False, 1): 0, (True, 0): 0, (True, 1): 0 }\n",
    "    for d, z, lbl in data:\n",
    "        group_counts[(z, lbl)] += 1\n",
    "    total = len(data)\n",
    "    group_probs = { g: float(count) / total for g, count in group_counts.items() }\n",
    "    \n",
    "    base_alpha = 1.2\n",
    "    alpha_adjustments = { (True, 1): 1.5, (True, 0): 1.3 }\n",
    "    \n",
    "    X, y_labels, sample_weights = [], [], []\n",
    "    for d, z, lbl in data:\n",
    "        X.append(p3feat(d))\n",
    "        y_labels.append(lbl)\n",
    "        alpha = alpha_adjustments.get((z, lbl), base_alpha)\n",
    "        w = 1.0 / (group_probs[(z, lbl)] ** alpha)\n",
    "        w = min(w, 100.0)\n",
    "        w = max(w, 1.0)\n",
    "        sample_weights.append(w)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300, \n",
    "        max_depth=10, \n",
    "        min_samples_leaf=20, \n",
    "        class_weight=None, \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X, y_labels, sample_weight=sample_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a36685-f817-4c5a-9d88-8874b369e84e",
   "metadata": {},
   "source": [
    "### Model-Based (In-processing) Intervention - Results\n",
    "#### Answers: How do different modeling choices impact fairness characteristics? Can you fix them with in-processing interventions?\n",
    "\n",
    "In our model-based intervention, we adjusted the training process by assigning different sample weights based on the group frequency of each (sensitive, label) combination. The goal was to force the model to pay more attention to underrepresented groups in the training data, ideally reducing unfairness. However, the results show that while the protected group (clients aged 40 or older) achieved a very high TPR of 0.878, the unprotected group (clients under 40) only reached a TPR of 0.698—a gap of 18 percentage points. Additionally, overall accuracy dropped to 63.3%. This indicates that the in-processing intervention, as currently configured, has not fixed the unfairness; in fact, it appears to have exacerbated the disparity between the groups. The weighting strategy may be overcompensating for the protected group, leading to a model that over-predicts positives for older clients while under-predicting for younger ones. Thus, while different modeling choices such as sample reweighting can have a direct impact on fairness metrics, achieving a balanced performance requires careful tuning of the in-processing method to avoid a significant drop in overall accuracy and an increased TPR gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143bec78-8519-4432-868b-a63e60200314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model-Based (In-Processing) Intervention ===\n",
      "Accuracy: 0.633\n",
      "TPR (Protected): 0.878\n",
      "TPR (Unprotected): 0.698\n",
      "TPR Difference: 0.180\n"
     ]
    }
   ],
   "source": [
    "    ###########################################\n",
    "    # Model-Based (In-Processing) Intervention Evaluation (p3)\n",
    "    ###########################################\n",
    "    clf_p3 = p3model(train_data)\n",
    "    X_test_p3 = [p3feat(d) for (d, z, _) in test_data]\n",
    "    y_pred_p3 = clf_p3.predict(X_test_p3)\n",
    "    \n",
    "    acc_p3 = accuracy_score(y_test, y_pred_p3)\n",
    "    tpr_prot_p3 = tpr([y_test[i] for i, z in enumerate(z_test) if z],\n",
    "                      [y_pred_p3[i] for i, z in enumerate(z_test) if z])\n",
    "    tpr_unprot_p3 = tpr([y_test[i] for i, z in enumerate(z_test) if not z],\n",
    "                        [y_pred_p3[i] for i, z in enumerate(z_test) if not z])\n",
    "    diff_p3 = abs(tpr_prot_p3 - tpr_unprot_p3)\n",
    "    \n",
    "    print(\"\\n=== Model-Based (In-Processing) Intervention ===\")\n",
    "    print(f\"Accuracy: {acc_p3:.3f}\")\n",
    "    print(f\"TPR (Protected): {tpr_prot_p3:.3f}\")\n",
    "    print(f\"TPR (Unprotected): {tpr_unprot_p3:.3f}\")\n",
    "    print(f\"TPR Difference: {diff_p3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7167d29-8993-47ef-b915-2df8539dc860",
   "metadata": {},
   "source": [
    "### Post-Processing Intervention - Code\n",
    "This function below implements a post-processing intervention that adjusts the final binary predictions by applying different decision thresholds to each sensitive group. First, it converts the test scores and sensitive attribute array into NumPy arrays. Then, it partitions the test scores into two groups—one for samples where the sensitive attribute is False (for example, \"younger\") and another where it is True (for example, \"older\"). For each group, the function computes the 50th percentile (median) of the scores, which serves as a threshold. Finally, for each test sample, if its score exceeds the threshold corresponding to its group, it is assigned a positive prediction (1); otherwise, a negative prediction (0). This method aims to mitigate fairness issues by adjusting the decision boundary separately for each group, so that any imbalance in score distributions is addressed in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f7624f-f35a-4747-85a7-419fde83297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 4. Post-Processing Intervention (p4)\n",
    "#########################################\n",
    "def p4labels(test_scores, zTest):\n",
    "    test_scores = np.array(test_scores)\n",
    "    zTest = np.array(zTest)\n",
    "    \n",
    "    # Partition scores by group\n",
    "    scores_group0 = test_scores[zTest == False]\n",
    "    scores_group1 = test_scores[zTest == True]\n",
    "    \n",
    "    # Define group-specific thresholds based on the 50th percentile\n",
    "    thr0 = np.percentile(scores_group0, 50)\n",
    "    thr1 = np.percentile(scores_group1, 50)\n",
    "    \n",
    "    preds = []\n",
    "    for s, z in zip(test_scores, zTest):\n",
    "        preds.append(1 if s > (thr1 if z else thr0) else 0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91726776-9899-4aff-835d-0f4fd49ee44b",
   "metadata": {},
   "source": [
    "### Post-Processing Intervention - Results\n",
    "#### Can you apply post-processing interventions to achieve desired fairness outcomes?\n",
    "\n",
    "The post-processing intervention code works by adjusting the decision thresholds separately for each group based on the median of the model’s predicted scores. In our results, after applying this intervention, the overall accuracy dropped to 57.5%, but the True Positive Rates (TPRs) for the protected group (older clients) and unprotected group (younger clients) became 83.8% and 82.2%, respectively—a TPR difference of only 1.6 percentage points. This demonstrates that post-processing can be effective in achieving a more balanced outcome across groups by fine-tuning the threshold for each group independently. However, while this method nearly equalizes the TPRs and thus significantly improves fairness in that metric, it comes with the trade-off of a substantial overall accuracy loss compared to the baseline model. In summary, post-processing interventions can indeed be applied to achieve desired fairness outcomes, but careful consideration must be given to the resulting trade-offs between fairness and overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3068a2-6452-49bc-b912-4ed3120ca4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-Processing Intervention ===\n",
      "Accuracy: 0.575\n",
      "TPR (Protected): 0.838\n",
      "TPR (Unprotected): 0.822\n",
      "TPR Difference: 0.016\n"
     ]
    }
   ],
   "source": [
    "    ###########################################\n",
    "    # Post-Processing Intervention Evaluation (p4)\n",
    "    ###########################################\n",
    "    test_scores = clf_baseline.predict_proba(X_test_p1)[:, 1]\n",
    "    y_pred_p4 = p4labels(test_scores, z_test)\n",
    "    \n",
    "    acc_p4 = accuracy_score(y_test, y_pred_p4)\n",
    "    tpr_prot_p4 = tpr([y_test[i] for i, z in enumerate(z_test) if z],\n",
    "                      [y_pred_p4[i] for i, z in enumerate(z_test) if z])\n",
    "    tpr_unprot_p4 = tpr([y_test[i] for i, z in enumerate(z_test) if not z],\n",
    "                        [y_pred_p4[i] for i, z in enumerate(z_test) if not z])\n",
    "    diff_p4 = abs(tpr_prot_p4 - tpr_unprot_p4)\n",
    "    \n",
    "    print(\"\\n=== Post-Processing Intervention ===\")\n",
    "    print(f\"Accuracy: {acc_p4:.3f}\")\n",
    "    print(f\"TPR (Protected): {tpr_prot_p4:.3f}\")\n",
    "    print(f\"TPR (Unprotected): {tpr_unprot_p4:.3f}\")\n",
    "    print(f\"TPR Difference: {diff_p4:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359e30-4a19-4a14-a380-b3ecaba45744",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "#### Answers: What types of interventions are most appropriate for your task (e.g. legal, practical to deploy, etc.)? What are the tradeoffs between them (e.g. how are other metrics negatively impacted by a particular intervention, etc.)\n",
    "\n",
    "The choice of fairness intervention depends heavily on the context and priorities of the task—in our case, predicting whether a client will subscribe to a bank term deposit while ensuring equitable treatment across age groups. Our experiments show that different methods yield very different trade-offs. For instance, the dataset-based intervention, which involves resampling to rebalance the distribution of positives and negatives across age groups, resulted in an overall accuracy of 85.8% but produced a true positive rate (TPR) of 66.8% for the older group (age ≥ 40) versus 61.5% for the younger group (age < 40), with a TPR difference of 5.4 percentage points. While this method is generally practical and easy to deploy—since it only requires manipulating the training data—it did not fully mitigate unfairness in our case; in fact, the gap widened compared to the baseline. This suggests that dataset-based interventions can sometimes be limited by the inherent behavior patterns captured in the data.\n",
    "\n",
    "In contrast, our model-based (in-processing) intervention, which adjusts sample weights during training based on the frequency of (sensitive, label) combinations, resulted in a dramatic drop in overall accuracy to 63.3% and an even larger TPR disparity (18 percentage points). Although in-processing methods have the theoretical advantage of incorporating fairness constraints directly into the training algorithm, they require careful tuning; otherwise, they may overcompensate for one group at the expense of overall performance. This accuracy loss and amplified gap indicate that without precise calibration, in-processing interventions might negatively impact other performance metrics and are less practical for production systems that need high overall accuracy.\n",
    "\n",
    "On the other hand, the post-processing intervention, which applies group-specific thresholds after model prediction, managed to reduce the TPR gap to just 1.6 percentage points, with TPRs of 83.8% and 82.2% for the older and younger groups respectively. However, this came at the cost of a significant decrease in overall accuracy (57.5%). While post-processing is relatively straightforward to implement—since it does not require retraining the model and can be applied as a “bolt-on” adjustment—its severe impact on accuracy and potentially on other metrics (like precision or false positive rate) raises concerns. Moreover, deploying different thresholds for different groups might raise legal or ethical questions in some contexts.\n",
    "\n",
    "In summary, each intervention carries its own set of trade-offs. The dataset-based approach is the most practical to deploy and does not require changing the model architecture, but its effectiveness depends on the underlying data distribution and may not always reduce unfairness. In-processing methods directly target model training but can cause substantial drops in overall performance if not tuned correctly. Post-processing techniques offer a quick fix for balancing metrics like TPR, yet they might undermine the model’s predictive power and lead to lower accuracy. Ultimately, the most appropriate intervention for our task will depend on the specific legal, operational, and business requirements—balancing the need for fairness with acceptable performance levels across all metrics. \n",
    "\n",
    "Based on our results, the baseline model already performs quite well from both an accuracy and fairness perspective—it achieves 85.3% accuracy with a TPR difference of only 2.7% between the older (protected) and younger (unprotected) groups. Although our dataset-based intervention was designed to rebalance the training data, it actually increased the TPR gap to 5.4% while achieving a similar overall accuracy (85.8%). The model-based intervention further widened the gap (18% difference) and significantly reduced accuracy, while the post-processing intervention nearly equalized TPRs (a 1.6% gap) but at the expense of a dramatic drop in accuracy to 57.5%.\n",
    "\n",
    "Given these trade-offs, the baseline model appears to be the most practical choice—it delivers high accuracy and a relatively minimal TPR difference, which is both legally defensible and operationally effective. In many legal and regulatory environments, a 2.7% disparity may be considered acceptable, especially when it comes with a strong overall performance. Therefore, in this scenario, we would choose not to deploy an additional fairness intervention because the baseline already strikes a good balance between accuracy and fairness.\n",
    "\n",
    "In our case, when fairness interventions such as dataset-based, in-processing, or post-processing adjustments are applied,they resulted in widening the TPR gap or significantly drop overall accuracy with the gap to be lowered, which is not desirable from an operational standpoint. In many real-world applications, especially in regulated financial contexts, a small disparity may be legally acceptable if overall performance remains high. Thus, we conclude that the baseline model naturally strikes a good balance between accuracy and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea869f1-0284-47ee-8faa-be6d6fdd0aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
